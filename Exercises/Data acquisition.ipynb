{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.1.\n",
    "\n",
    "The list below contains a number of URLs. They are the web addresses of texts created for the [Project Gutenberg](https://www.gutenberg.org) website.\n",
    "\n",
    "```\n",
    "urls = [ 'https://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'https://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'https://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'https://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "```\n",
    "\n",
    "Write a program in Python that downloads all the files in this list and stores them in the current directory.\n",
    "As filenames, use the same names that are used by Project Gutenberg (e.g. '580-0.txt' or '1400-0.txt').\n",
    "The basename in a URL can be extracted using the [`os.path.basename()`](https://docs.python.org/3/library/os.path.html#os.path.basename) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os \n",
    "\n",
    "# Recreate the given list using copy and paste\n",
    "urls = [  \n",
    "]\n",
    "\n",
    "# We use a for-loop to take the same steps for each item in the list:\n",
    "for url in urls:\n",
    "    # 1. Download the file contents\n",
    "    \n",
    "    # 1a. Force the textual contents to be interpreted as UTF-8 encoded, because the website does not send the text encoding\n",
    "    \n",
    "    # 2. Use basename to get a suitable filename\n",
    "    \n",
    "    # 3. Open the file in write mode and write the downloaded file contents to the file\n",
    "    \n",
    "    # 4. Close the file\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.2.\n",
    "\n",
    "Write Python code which can download the titles and the URLs of Wikipedia articles whose titles contain the word 'Dutch'. Your code needs to display the first 30 results only.\n",
    "\n",
    "*Hint: the tutorial covers the Wikipedia API.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "baseURL = 'https://en.wikipedia.org/w/api.php?action=opensearch'\n",
    "\n",
    "# Get the search results and display them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.3.\n",
    "\n",
    "Write an application in Python that extracts all the publications that have been added to a specific ORCID account, using the ORCID API.\n",
    "\n",
    "Information about individual ORCID accounts can be obtained by appending their ID to the base URL <https://pub.orcid.org/v2.0/>. The ORCID API returns data in XML by default. In the XML, the list of publications can be found using the XPath `r:record/a:activities-summary/a:works/a:group` (using the namespace declarations given below).\n",
    "\n",
    "*Note: we use the [ElementTree](https://docs.python.org/3/library/xml.etree.elementtree.html) library to process the XML data. It is very powerful, but has a quite steep learning curve.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose an ORCID to look up, e.g. 0000-0002-8469-6804\n",
    "orcid = ''\n",
    "\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "ns = {'o': 'http://www.orcid.org/ns/orcid' ,\n",
    "'s' : 'http://www.orcid.org/ns/search' ,\n",
    "'h': 'http://www.orcid.org/ns/history' ,\n",
    "'p': 'http://www.orcid.org/ns/person' ,\n",
    "'pd': 'http://www.orcid.org/ns/personal-details' ,\n",
    "'a': 'http://www.orcid.org/ns/activities' ,\n",
    "'e': 'http://www.orcid.org/ns/employment' ,\n",
    "'c': 'http://www.orcid.org/ns/common' , \n",
    "'w': 'http://www.orcid.org/ns/work'}\n",
    "\n",
    "# We expect that there may be an error and therefore use `try` and `except`\n",
    "try:\n",
    "    # Construct the API call\n",
    "    orcidUrl = \"https://pub.orcid.org/v2.0/\" + orcid\n",
    "    print( orcidUrl )\n",
    "    \n",
    "    # Find and print the record creation date\n",
    "    \n",
    "    \n",
    "    # Find and print the titles of the publications\n",
    "\n",
    "            \n",
    "except:\n",
    "    print(\"Data could not be downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.4.\n",
    "\n",
    "The API developed by [OpenStreetMap](https://www.openstreetmap.org/) can be used, among other things, to find the precise geographic coordinates of a specific location. The base URL of this API is https://nominatim.openstreetmap.org/search. Following the `q` parameter, you need to supply a string describing the locations whose latitude and longitude you want to find. As values for the `format` parameter, you can use `xml` for XML-formatted data or `json` for JSON-formatted data. Use this API to find the longitude and the latitude of the addresses in the following list:\n",
    "\n",
    "```\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "# Create the list of addresses\n",
    "addresses = []\n",
    "\n",
    "# Look up the latitude and longitude\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.5.\n",
    "\n",
    "Extract the titles of all the movies which are included on the [list of top rated movies](https://www.imdb.com/chart/top?ref_=ft_250) using web scraping. Also extract the URL of the webpage on IMDB describing these movies.\n",
    "\n",
    "*Hint: you will need to find the location of the information within the HTML elements and attributes by hand. Then you can programmatically extract the information.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
